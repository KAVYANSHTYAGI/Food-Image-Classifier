{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQNNZzLoZhJ1ZpZa5tMbRr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KAVYANSHTYAGI/Food-Image-Classifier/blob/main/novel_architecture_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5RhdJ2Xvlzmr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau , LearningRateScheduler\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AQRFBH_omAmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9c3a83-ef0c-48b3-8d0d-ebbc9ecad532"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_dir_training = \"/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training\"\n",
        "class_labels_training = os.listdir(base_dir_training)\n",
        "print(len(class_labels_training))\n",
        "\n",
        "base_dir_testing = \"/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing\"\n",
        "class_labels_testing = os.listdir(base_dir_testing)\n",
        "print(len(class_labels_testing))"
      ],
      "metadata": {
        "id": "l7oyWLadmC3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79fde0ed-a0d5-4db2-9050-7a98762dbdd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "\n",
        "data_training = []\n",
        "count = 0\n",
        "\n",
        "for label in class_labels_training:\n",
        "    path = os.path.join(base_dir_training, label)\n",
        "    print(path)\n",
        "    for img in os.listdir(path):\n",
        "        try:\n",
        "            image = load_img(os.path.join(path,img), color_mode=\"rgb\", target_size=(224,224))\n",
        "            image = img_to_array(image)\n",
        "            image /= 255.0\n",
        "            data_training.append([image, count])\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    count = count + 1\n",
        "\n",
        "#testing\n",
        "\n",
        "data_testing = []\n",
        "count = 0\n",
        "\n",
        "for label in class_labels_testing:\n",
        "    path = os.path.join(base_dir_testing, label)\n",
        "    print(path)\n",
        "    for img in os.listdir(path):\n",
        "        try:\n",
        "            image = load_img(os.path.join(path,img), color_mode=\"rgb\", target_size=(224,224))\n",
        "            image = img_to_array(image)\n",
        "            image /= 255.0\n",
        "            data_testing.append([image, count])\n",
        "\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    count = count + 1\n"
      ],
      "metadata": {
        "id": "WqkzPocZmENq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b540aa-9669-476f-f8e3-537e6db58b87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Aloo Puri\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/bal mithai - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Bhindi Masala\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Chhole Bhature\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/chole kulcha food images - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Dal Makhni\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/egg bhurji - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/ghewar - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Gulab Jamun\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/kathi roll - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/kulfi - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/lassi - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/malai kofta\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/masala papad - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/matar paneer - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/mix veg\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/momos\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/naan\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/palak panner\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/paneer tikka\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/pav bhaji images - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/prawn curry indian - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Rajma Chawal\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/rasmalai images - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/red sauce pasta - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/Samosa\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/spaghetti - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/sweet masala corn - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/veg sandwich - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/training/white sauce pasta - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Aloo Puri\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/bal mithai - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Bhindi Masala\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Chhole Bhature\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/chole kulcha food images - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Dal Makhni\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/egg bhurji - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/ghewar - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Gulab Jamun\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/kathi roll - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/kulfi - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/lassi - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/malai kofta\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/masala papad - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/matar paneer - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/mix veg\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/momos\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/naan\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/palak panner\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/paneer tikka\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/pav bhaji images - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/prawn curry indian - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Rajma Chawal\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/rasmalai images - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/red sauce pasta - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/Samosa\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/spaghetti - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/sweet masala corn - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/veg sandwich - Google Search\n",
            "/content/drive/MyDrive/Food Image Datasets/North Indian/main_final/testing/white sauce pasta - Google Search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_training))\n",
        "print(len(data_testing))\n",
        "print(count)\n",
        "num_classes = count"
      ],
      "metadata": {
        "id": "-MOCEvZimYWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4683508e-86e2-4d5f-d967-c666e53f606e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8039\n",
            "2027\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,y_train = zip(*data_training)\n",
        "\n",
        "x_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "\n",
        "X_test,y_test = zip(*data_testing)\n",
        "\n",
        "x_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "p7sIgVvhmGXr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "pAu1u4PSmGU6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=90,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'               # Fill missing pixels after transformations\n",
        ")\n",
        "\n",
        "\n",
        "val_test_datagen = ImageDataGenerator()\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow(x_train, y_train)\n",
        "val_generator = val_test_datagen.flow(x_test, y_test)"
      ],
      "metadata": {
        "id": "U4fToGqwmGSa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def multi_input_dataset(generator, num_inputs):\n",
        "    \"\"\"\n",
        "    Converts an ImageDataGenerator into a tf.data.Dataset that supports multiple inputs.\n",
        "\n",
        "    Args:\n",
        "    - generator: The ImageDataGenerator that yields (images, labels).\n",
        "    - num_inputs: Number of model inputs.\n",
        "\n",
        "    Returns:\n",
        "    - A tf.data.Dataset that provides multiple inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def _generator():\n",
        "        for batch_images, batch_labels in generator:\n",
        "            # Duplicate input for all three branches\n",
        "            yield [batch_images] * num_inputs, batch_labels\n",
        "\n",
        "    # ✅ Correct way to get the number of classes\n",
        "    num_classes = len(generator.class_indices)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        _generator,\n",
        "        output_signature=(\n",
        "            [tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32)] * num_inputs,\n",
        "            tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)  # ✅ Uses `len(generator.class_indices)`\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "rd7RsX1Tl2fb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of model inputs\n",
        "NUM_INPUTS = 3\n",
        "\n",
        "def data_generator(x_data, y_data):\n",
        "    \"\"\" Generator that yields images correctly for multi-input models. \"\"\"\n",
        "    for img, label in zip(x_data, y_data):\n",
        "        yield (img, img, img), label  # Three separate inputs, NOT an extra dimension\n",
        "\n"
      ],
      "metadata": {
        "id": "iWSOULF8w3jp"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_multi_input = None\n",
        "val_multi_input = None"
      ],
      "metadata": {
        "id": "RpUsqRtbxh9J"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input shape dynamically\n",
        "input_shape = (224, 224, 3)  # Ensure correct shape\n",
        "num_classes = y_train.shape[-1]\n",
        "\n",
        "# Define dataset correctly\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(x_train, y_train),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=input_shape, dtype=tf.float32),  # Input 1\n",
        "         tf.TensorSpec(shape=input_shape, dtype=tf.float32),  # Input 2\n",
        "         tf.TensorSpec(shape=input_shape, dtype=tf.float32)),  # Input 3\n",
        "        tf.TensorSpec(shape=(num_classes,), dtype=tf.float32)  # Labels\n",
        "    )\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(x_test, y_test),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=input_shape, dtype=tf.float32),  # Input 1\n",
        "         tf.TensorSpec(shape=input_shape, dtype=tf.float32),  # Input 2\n",
        "         tf.TensorSpec(shape=input_shape, dtype=tf.float32)),  # Input 3\n",
        "        tf.TensorSpec(shape=(num_classes,), dtype=tf.float32)  # Labels\n",
        "    )\n",
        ")\n",
        "\n",
        "# Batch and optimize performance\n",
        "train_multi_input = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "val_multi_input = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "FNI3QbYel7Ka"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_multi_input.take(1):\n",
        "    print(f\"Texture Input Shape: {images[0].shape}\")  # Should be (batch_size, 224, 224, 3)\n",
        "    print(f\"Color Input Shape: {images[1].shape}\")    # Should be (batch_size, 224, 224, 3)\n",
        "    print(f\"Structure Input Shape: {images[2].shape}\") # Should be (batch_size, 224, 224, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaUOQ6ckwTuQ",
        "outputId": "dd4956ac-0610-4950-a222-41a695150299"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texture Input Shape: (64, 224, 224, 3)\n",
            "Color Input Shape: (64, 224, 224, 3)\n",
            "Structure Input Shape: (64, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "9FYSjeEzm72i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define three separate inputs\n",
        "texture_input = Input(shape=(224, 224, 3), name=\"texture_input\")\n",
        "color_input = Input(shape=(224, 224, 3), name=\"color_input\")\n",
        "structure_input = Input(shape=(224, 224, 3), name=\"structure_input\")\n"
      ],
      "metadata": {
        "id": "oshjIfhZ2UjA"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Conv2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, Multiply, Reshape, Add, Activation\n",
        "\n",
        "class CBAMBlock(Layer):\n",
        "    def __init__(self, ratio=8, **kwargs):\n",
        "        super(CBAMBlock, self).__init__(**kwargs)\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channel = input_shape[-1]\n",
        "\n",
        "        # Channel Attention Layers\n",
        "        self.avg_dense1 = Dense(channel // self.ratio, activation='relu', use_bias=False)\n",
        "        self.avg_dense2 = Dense(channel, activation='sigmoid', use_bias=False)\n",
        "\n",
        "        self.max_dense1 = Dense(channel // self.ratio, activation='relu', use_bias=False)\n",
        "        self.max_dense2 = Dense(channel, activation='sigmoid', use_bias=False)\n",
        "\n",
        "        # Spatial Attention Layer\n",
        "        self.spatial_conv = Conv2D(1, kernel_size=7, padding=\"same\", activation='sigmoid', use_bias=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Channel Attention\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = GlobalMaxPooling2D()(inputs)\n",
        "\n",
        "        avg_dense = self.avg_dense2(self.avg_dense1(avg_pool))\n",
        "        max_dense = self.max_dense2(self.max_dense1(max_pool))\n",
        "\n",
        "        channel_attention = Add()([avg_dense, max_dense])\n",
        "        channel_attention = Activation('sigmoid')(channel_attention)\n",
        "        channel_attention = Multiply()([inputs, channel_attention])\n",
        "\n",
        "        # Spatial Attention\n",
        "        avg_pool = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n",
        "        max_pool = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n",
        "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
        "\n",
        "        spatial_attention = self.spatial_conv(concat)\n",
        "        spatial_attention = Multiply()([channel_attention, spatial_attention])\n",
        "\n",
        "        return spatial_attention\n"
      ],
      "metadata": {
        "id": "ADa4YvGHiUgr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Branch 1: Texture Branch (EfficientNetB4 + CBAM Attention)\n",
        "Purpose: Local texture extraction\n",
        "Model: EfficientNetB4 (CNN backbone)"
      ],
      "metadata": {
        "id": "NIhFH7yFh4yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D, Multiply, Add, Activation\n",
        "\n",
        "# Load Pretrained EfficientNetB4 (Feature Extractor)\n",
        "texture_base = EfficientNetB4(include_top=False, weights='imagenet')(texture_input)\n",
        "\n",
        "# ✅ Apply CBAM Attention (Custom Block)\n",
        "class CBAMBlock(layers.Layer):\n",
        "    def __init__(self, ratio=8, **kwargs):\n",
        "        super(CBAMBlock, self).__init__(**kwargs)\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channel = input_shape[-1]\n",
        "\n",
        "        # Channel Attention\n",
        "        self.avg_dense1 = Dense(channel // self.ratio, activation='relu', use_bias=False)\n",
        "        self.avg_dense2 = Dense(channel, activation='sigmoid', use_bias=False)\n",
        "\n",
        "        self.max_dense1 = Dense(channel // self.ratio, activation='relu', use_bias=False)\n",
        "        self.max_dense2 = Dense(channel, activation='sigmoid', use_bias=False)\n",
        "\n",
        "        # Spatial Attention\n",
        "        self.spatial_conv = Conv2D(1, kernel_size=7, padding=\"same\", activation='sigmoid', use_bias=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Channel Attention\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = GlobalMaxPooling2D()(inputs)\n",
        "\n",
        "        avg_dense = self.avg_dense2(self.avg_dense1(avg_pool))\n",
        "        max_dense = self.max_dense2(self.max_dense1(max_pool))\n",
        "\n",
        "        channel_attention = Add()([avg_dense, max_dense])\n",
        "        channel_attention = Activation('sigmoid')(channel_attention)\n",
        "        channel_attention = Multiply()([inputs, channel_attention])\n",
        "\n",
        "        # Spatial Attention\n",
        "        avg_pool = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n",
        "        max_pool = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n",
        "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
        "\n",
        "        spatial_attention = self.spatial_conv(concat)\n",
        "        spatial_attention = Multiply()([channel_attention, spatial_attention])\n",
        "\n",
        "        return spatial_attention\n",
        "\n",
        "# Apply CBAM Block\n",
        "texture_attn = CBAMBlock()(texture_base)\n",
        "\n",
        "# ✅ Global Pooling + Dense Layer\n",
        "texture_x = GlobalAveragePooling2D()(texture_attn)\n",
        "texture_x = Dense(256, activation='relu')(texture_x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARekRaUOh0mb"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Branch 2: Color-Focused Branch (ViT-based Transformer backbone)\n",
        "Instead of convolutional layers, use a pretrained Vision Transformer (ViT Small or Swin Transformer) to capture rich color and global context features effectively:"
      ],
      "metadata": {
        "id": "xwDyoC09h6ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "# Load Pretrained ViT Model\n",
        "vit_model = TFAutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# ✅ Fix: Convert Input to Channel-First Format for ViT\n",
        "def vit_feature_extraction(image):\n",
        "    image = tf.transpose(image, [0, 3, 1, 2])  # Convert (B, H, W, C) → (B, C, H, W)\n",
        "    outputs = vit_model(image, training=False)\n",
        "    return outputs.last_hidden_state  # Extract features\n",
        "\n",
        "# ✅ Wrap Inside Lambda Layer with Correct Output Shape\n",
        "color_features = tf.keras.layers.Lambda(\n",
        "    lambda x: vit_feature_extraction(x),\n",
        "    output_shape=(197, 768)  # Ensure correct output shape\n",
        ")(color_input)\n",
        "\n",
        "# ✅ Pooling + Dense Layer\n",
        "color_x = GlobalAveragePooling1D()(color_features)\n",
        "color_x = Dense(256, activation='relu')(color_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80DN9gY3h676",
        "outputId": "80871ceb-89a7-4797-9796-3df868c743cc"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFViTModel.\n",
            "\n",
            "All the weights of TFViTModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Branch 3: Structure Branch (Swin Transformer) (Highly Novel & High-performing)\n",
        "Swin Transformer is highly effective for global context and capturing complex image-level interactions, perfect for your use case:\n",
        "\n",
        "Implementation (using pretrained Swin Transformer from Hugging Face):"
      ],
      "metadata": {
        "id": "WVnfYWjlh_IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# Load Pretrained Swin Transformer\n",
        "swin_model = TFAutoModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "# Define Input for Structure Branch\n",
        "structure_input = tf.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "# ✅ Force Swin Transformer to Run in Eager Execution Mode\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def swin_feature_extraction(image):\n",
        "    image = tf.transpose(image, [0, 3, 1, 2])  # Convert to (B, C, H, W)\n",
        "    outputs = swin_model(image, training=False)\n",
        "    return outputs.last_hidden_state  # Extract features\n",
        "\n",
        "# ✅ Ensure Swin Transformer runs in Pure Eager Mode using `tf.py_function()`\n",
        "structure_features = Lambda(\n",
        "    lambda x: tf.py_function(swin_feature_extraction, [x], Tout=tf.float32),\n",
        "    output_shape=(49, 768)  # Manually define the expected shape\n",
        ")(structure_input)\n",
        "\n",
        "# ✅ Pooling + Dense Layer\n",
        "structure_x = tf.keras.layers.GlobalAveragePooling1D()(structure_features)\n",
        "structure_x = tf.keras.layers.Dense(256, activation='relu')(structure_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzAy6ygWh_Vq",
        "outputId": "30d6f58b-ff02-4b91-f992-cf4993008f48"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFSwinModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing TFSwinModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFSwinModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFSwinModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFSwinModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_multi_input.take(1):\n",
        "    print(f\"Texture Input Shape: {images[0].shape}\")   # (64, 224, 224, 3)\n",
        "    print(f\"Color Input Shape: {images[1].shape}\")     # (64, 224, 224, 3)\n",
        "    print(f\"Structure Input Shape (Swin): {images[2].shape}\") # (64, 224, 224, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dAJL63myJqo",
        "outputId": "8ba1253a-4326-4e88-8b0d-71b100cdfa7a"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texture Input Shape: (64, 224, 224, 3)\n",
            "Color Input Shape: (64, 224, 224, 3)\n",
            "Structure Input Shape (Swin): (64, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of attention_features BEFORE Reshape:\", attention_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J71NqYyD3e7o",
        "outputId": "ac4476a9-da6b-4d2d-8051-cfacfbc6e55b"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention_features BEFORE Reshape: (None, 1, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Concatenate, MultiHeadAttention, Flatten, Dropout\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# ✅ Concatenate Features from All Three Branches\n",
        "combined_features = Concatenate()([texture_x, color_x, structure_x])\n",
        "\n",
        "expanded_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(combined_features)\n",
        "\n",
        "# ✅ Apply Multi-Head Attention\n",
        "attention_features = MultiHeadAttention(num_heads=4, key_dim=64)(\n",
        "    query=expanded_features,\n",
        "    value=expanded_features,\n",
        "    key=expanded_features\n",
        ")\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "\n",
        "attention_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(attention_features)\n",
        "\n",
        "print(\"Shape of attention_features BEFORE Pooling:\", attention_features.shape)\n",
        "\n",
        "# Fix shape issues before pooling\n",
        "if len(attention_features.shape) == 2:  # (batch_size, feature_dim)\n",
        "    attention_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(attention_features)\n",
        "elif len(attention_features.shape) == 4:  # (batch_size, 1, 1, feature_dim)\n",
        "    attention_features = Lambda(lambda x: tf.squeeze(x, axis=1))(attention_features)\n",
        "\n",
        "# Apply Global Average Pooling\n",
        "fusion_output = GlobalAveragePooling1D()(attention_features)\n",
        "print(\"Shape of fusion_output AFTER Pooling:\", fusion_output.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Fully Connected Layers\n",
        "x = Flatten()(fusion_output)  # ✅ Ensure it flattens correctly\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(256, activation='relu')(x)  # ✅ Will no longer have shape issues\n",
        "x = Dropout(0.2)(x)\n",
        "output = Dense(30, activation='softmax')(x)  # Final classification layer (30 classes)\n",
        "  # Final classification layer (30 classes)\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# ✅ Create Multi-Input Model\n",
        "model = Model(inputs=[texture_input, color_input, structure_input], outputs=output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8YX4sMDlTBj",
        "outputId": "06025e29-fc20-47cc-e5a1-5b29dcb9e23e"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of attention_features BEFORE Pooling: (None, 1, 1, 768)\n",
            "Shape of fusion_output AFTER Pooling: (None, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "optimizer = AdamW(learning_rate=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=['accuracy', Precision(), Recall()]\n",
        ")\n"
      ],
      "metadata": {
        "id": "LPJjqIWMlCji"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# Stop training when validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Reduce learning rate if validation loss plateaus\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Save the best model based on validation loss\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Callback List\n",
        "callbacks = [early_stopping, reduce_lr, checkpoint]\n"
      ],
      "metadata": {
        "id": "3FKIaVR6ldXj"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_multi_input,\n",
        "    epochs=50,\n",
        "    validation_data=val_multi_input,\n",
        "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
        "    shuffle=True)\n",
        "\n",
        "# Plotting training and validation loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xMp7uTe9mGQa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "861d3371-4072-4a49-a839-827bc36c5243"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mas_list() is not defined on an unknown TensorShape.\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=('tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)')\n  • training=True\n  • mask=('None', 'None', 'None')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-3753b6b97c2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_multi_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_multi_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mas_list() is not defined on an unknown TensorShape.\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=('tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(None, 224, 224, 3), dtype=float32)')\n  • training=True\n  • mask=('None', 'None', 'None')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy, precision, recall, f1_score = model.evaluate(val_generator)\n",
        "print(f'test Loss: {loss}, test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "6AJG7WComGNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define three input branches\n",
        "model = tf.keras.Model(\n",
        "    inputs=[texture_input, color_input, structure_input],  # ✅ Three separate inputs\n",
        "    outputs=output\n",
        ")\n",
        "\n",
        "# ✅ Print Model Summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "bkXiA5YYmGLb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fafbd39-1c61-4eac-ca69-fd9690eebff7"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ texture_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ efficientnetb4            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1792\u001b[0m)     │     \u001b[38;5;34m17,673,823\u001b[0m │ texture_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ color_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_19            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cbam_block_1 (\u001b[38;5;33mCBAMBlock\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1792\u001b[0m)     │      \u001b[38;5;34m1,605,730\u001b[0m │ efficientnetb4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_14 (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ color_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_24 (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m768\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ input_layer_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ cbam_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ lambda_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ lambda_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m459,008\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m196,864\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_44 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m196,864\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_17            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ dense_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ dense_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_35 (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ concatenate_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_19   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │        \u001b[38;5;34m787,968\u001b[0m │ lambda_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ lambda_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ lambda_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_36 (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m768\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_37 (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m768\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lambda_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ lambda_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_39 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ flatten_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m196,864\u001b[0m │ dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_40 (\u001b[38;5;33mDropout\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │          \u001b[38;5;34m7,710\u001b[0m │ dropout_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ texture_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ efficientnetb4            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> │ texture_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ color_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_19            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ cbam_block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CBAMBlock</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,730</span> │ efficientnetb4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ color_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cbam_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">459,008</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_17            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ dense_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ dense_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_19   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">787,968</span> │ lambda_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ lambda_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ lambda_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lambda_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling1d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">7,710</span> │ dropout_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,124,831\u001b[0m (80.58 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,124,831</span> (80.58 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,999,624\u001b[0m (80.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,999,624</span> (80.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m125,207\u001b[0m (489.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,207</span> (489.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "def prepare_multi_input(image, label):\n",
        "    return (image, image, image), label\n",
        "train_multi_input = train_dataset.map(prepare_multi_input).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "val_multi_input = val_dataset.map(prepare_multi_input).batch(64).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "emO1MI529OQs"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGR7cyJkJFz9",
        "outputId": "30314222-20d8-4db5-af8d-8194f82391e5"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8039, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0rk8S0OKBPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SavVuOQKBMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qm8oRKKBKBKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, Flatten, Concatenate, GlobalAveragePooling2D,\n",
        "    GlobalAveragePooling1D, Lambda, MultiHeadAttention\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from transformers import TFAutoModel\n",
        "\n",
        "# =======================\n",
        "# Texture Branch\n",
        "# =======================\n",
        "texture_input = Input(shape=(224, 224, 3), name=\"texture_input\")\n",
        "# Pretrained EfficientNetB4 feature extractor (without top)\n",
        "texture_base = EfficientNetB4(include_top=False, weights=\"imagenet\")(texture_input)\n",
        "\n",
        "# Define a CBAM block as a custom layer\n",
        "class CBAMBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, ratio=8, **kwargs):\n",
        "        super(CBAMBlock, self).__init__(**kwargs)\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channel = input_shape[-1]\n",
        "        # Channel Attention layers\n",
        "        self.avg_dense1 = Dense(channel // self.ratio, activation='relu', use_bias=False)\n",
        "        self.avg_dense2 = Dense(channel, activation='sigmoid', use_bias=False)\n",
        "        self.max_dense1 = Dense(channel // self.ratio, activation='relu', use_bias=False)\n",
        "        self.max_dense2 = Dense(channel, activation='sigmoid', use_bias=False)\n",
        "        # Spatial Attention layer\n",
        "        self.spatial_conv = tf.keras.layers.Conv2D(1, kernel_size=7, padding=\"same\",\n",
        "                                                   activation=\"sigmoid\", use_bias=False)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Channel Attention\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling2D()(inputs)\n",
        "        avg_dense = self.avg_dense2(self.avg_dense1(avg_pool))\n",
        "        max_dense = self.max_dense2(self.max_dense1(max_pool))\n",
        "        channel_attention = tf.keras.layers.Add()([avg_dense, max_dense])\n",
        "        channel_attention = tf.keras.layers.Activation(\"sigmoid\")(channel_attention)\n",
        "        channel_attention = tf.keras.layers.Multiply()([inputs, channel_attention])\n",
        "        # Spatial Attention\n",
        "        avg_pool_spatial = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n",
        "        max_pool_spatial = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n",
        "        concat = tf.concat([avg_pool_spatial, max_pool_spatial], axis=-1)\n",
        "        spatial_attention = self.spatial_conv(concat)\n",
        "        spatial_attention = tf.keras.layers.Multiply()([channel_attention, spatial_attention])\n",
        "        return spatial_attention\n",
        "\n",
        "# Apply CBAM to the EfficientNetB4 features\n",
        "texture_attn = CBAMBlock()(texture_base)\n",
        "texture_x = GlobalAveragePooling2D()(texture_attn)\n",
        "texture_x = Dense(256, activation=\"relu\")(texture_x)  # Final texture feature vector\n",
        "\n",
        "# =======================\n",
        "# Color Branch (ViT)\n",
        "# =======================\n",
        "color_input = Input(shape=(224, 224, 3), name=\"color_input\")\n",
        "# Load pre-trained ViT model from Hugging Face\n",
        "vit_model = TFAutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "def vit_feature_extraction(image):\n",
        "    # Convert input from (B, H, W, C) to (B, C, H, W)\n",
        "    image = tf.transpose(image, [0, 3, 1, 2])\n",
        "    outputs = vit_model(image, training=False)\n",
        "    return outputs.last_hidden_state  # Expected shape: (batch_size, 197, 768)\n",
        "\n",
        "# Wrap in a Lambda using tf.py_function (note: output shape not statically inferred)\n",
        "color_features = Lambda(\n",
        "    lambda x: tf.py_function(vit_feature_extraction, [x], tf.float32),\n",
        "    name=\"vit_features\"  # Optionally, specify output_shape=(197,768) if known\n",
        ")(color_input)\n",
        "color_x = GlobalAveragePooling1D()(color_features)\n",
        "color_x = Dense(256, activation=\"relu\")(color_x)  # Final color feature vector\n",
        "\n",
        "# =======================\n",
        "# Structure Branch (Swin Transformer)\n",
        "# =======================\n",
        "structure_input = Input(shape=(224, 224, 3), name=\"structure_input\")\n",
        "# Load pre-trained Swin Transformer from Hugging Face\n",
        "swin_model = TFAutoModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def swin_feature_extraction(image):\n",
        "    # Convert input from (B, H, W, C) to (B, C, H, W)\n",
        "    image = tf.transpose(image, [0, 3, 1, 2])\n",
        "    outputs = swin_model(image, training=False)\n",
        "    return outputs.last_hidden_state  # Expected shape: (batch_size, 49, 768)\n",
        "\n",
        "# Wrap in a Lambda using tf.py_function\n",
        "structure_features = Lambda(\n",
        "    lambda x: tf.py_function(swin_feature_extraction, [x], tf.float32),\n",
        "    name=\"swin_features\"  # Optionally, specify output_shape=(49,768)\n",
        ")(structure_input)\n",
        "structure_x = GlobalAveragePooling1D()(structure_features)\n",
        "structure_x = Dense(256, activation=\"relu\")(structure_x)  # Final structure feature vector\n",
        "\n",
        "# =======================\n",
        "# Fusion Block\n",
        "# =======================\n",
        "# Concatenate the three 256-d feature vectors → shape (batch_size, 768)\n",
        "combined_features = Concatenate(name=\"combined_features\")([texture_x, color_x, structure_x])\n",
        "\n",
        "# For fusion with MultiHeadAttention, we need a sequence dimension.\n",
        "# Expand dims: (batch_size, 768) → (batch_size, 1, 768)\n",
        "expanded_features = Lambda(lambda x: tf.expand_dims(x, axis=1), name=\"expand_dims\")(combined_features)\n",
        "\n",
        "# Apply Multi-Head Attention.\n",
        "# Here we use key_dim=64 (so total output dimension is 64*num_heads, e.g., 256 if num_heads=4)\n",
        "attention_features = MultiHeadAttention(num_heads=4, key_dim=64, name=\"multi_head_attention\")(\n",
        "    query=expanded_features,\n",
        "    value=expanded_features,\n",
        "    key=expanded_features\n",
        ")\n",
        "# Squeeze back the sequence dimension: (batch_size, 1, 768) → (batch_size, 768)\n",
        "squeezed_features = Lambda(lambda x: tf.squeeze(x, axis=1), name=\"squeeze_dims\")(attention_features)\n",
        "\n",
        "# Use the fused vector directly as the fusion output\n",
        "fusion_output = squeezed_features\n",
        "\n",
        "# =======================\n",
        "# Classification Head\n",
        "# =======================\n",
        "x = Flatten(name=\"flatten\")(fusion_output)  # (batch_size, 768)\n",
        "x = Dropout(0.4, name=\"dropout_1\")(x)\n",
        "x = Dense(256, activation=\"relu\", name=\"dense_1\")(x)\n",
        "x = Dropout(0.2, name=\"dropout_2\")(x)\n",
        "output = Dense(30, activation=\"softmax\", name=\"predictions\")(x)  # 30 classes\n",
        "\n",
        "# =======================\n",
        "# Build & Compile Model\n",
        "# =======================\n",
        "model = Model(inputs=[texture_input, color_input, structure_input], outputs=output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "VOMt1dulKBHc",
        "outputId": "95bd7b09-c2d7-46da-ff77-464e12a629ba"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFViTModel.\n",
            "\n",
            "All the weights of TFViTModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, name=color_input>',)\n  • kwargs={'mask': 'None'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-48b9c12acd04>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# Wrap in a Lambda using tf.py_function (note: output shape not statically inferred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m color_features = Lambda(\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_feature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"vit_features\"\u001b[0m  \u001b[0;31m# Optionally, specify output_shape=(197,768) if known\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0;34m\"We could not automatically infer the shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;34m\"the Lambda's output. Please specify the `output_shape` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 224, 224, 3), dtype=float32, sparse=False, name=color_input>',)\n  • kwargs={'mask': 'None'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtzYEhBwKBFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oppK4zlQKBCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rO8tGnMWKBAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5K-L36xKA9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}